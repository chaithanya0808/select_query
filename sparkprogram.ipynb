{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f15d0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using packages ['org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0', 'org.apache.kafka:kafka-clients:3.5.1']\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "from kafka import  KafkaProducer\n",
    "\n",
    "\n",
    "\n",
    "scala_version = '2.12'  # TODO: Ensure this is correct\n",
    "spark_version = '3.3.0'\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.5.1'\n",
    "]\n",
    "\n",
    "args = os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "if not args:\n",
    "    args = f'--packages {\",\".join(packages)}'\n",
    "    print('Using packages', packages)\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f'{args} pyspark-shell'\n",
    "else:\n",
    "    print(f'Found existing args: {args}')\n",
    "    \n",
    "    \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1dcf13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Java home\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk1.8.0_251\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3824719",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"ETLPipeline\") \\\n",
    "    .setMaster(\"local\") \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"c:/pyspark/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f72f6d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "etl = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0f4803f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=\"chaithanya\"\n",
    "pwd=\"12345678\"\n",
    "\n",
    "#sql db details\n",
    "server = \"localhost\"\n",
    "src_db = \"prac\"\n",
    "src_driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba54ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_url = f\"jdbc:sqlserver://{server}:1433;databaseName={src_db};user={uid};password={pwd};\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3c1972c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"select emp_id,emp_name,salary,manager_id from prac.dbo.emp_manager\"\"\"\n",
    "table=\"emp_manager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77e42134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----------+\n",
      "|emp_id|emp_name|salary|manager_id|\n",
      "+------+--------+------+----------+\n",
      "|     1|   Ankit| 10000|         4|\n",
      "|     2|   Mohit| 15000|         5|\n",
      "|     3|   Vikas| 10000|         4|\n",
      "|     4|   Rohit|  5000|         2|\n",
      "|     5|   Mudit| 12000|         6|\n",
      "|     6|    Agam| 12000|         2|\n",
      "|     7|  Sanjay|  9000|         2|\n",
      "|     8|  Ashish|  5000|         2|\n",
      "+------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=etl.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    options(driver=src_driver, user=uid, password=pwd, url=src_url,query=query). \\\n",
    "    load()\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81b135dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_bootstrap_servers = \"localhost:9092\"\n",
    "kafka_topic = \"quickstart-events4\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a087a4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- emp_id: integer (nullable = true)\n",
      " |-- emp_name: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- manager_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "dfs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c925baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = dfs.selectExpr(\"CAST(emp_id AS STRING) AS key\", \"to_json(struct(*)) AS value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b53efc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'quickstart-events4'\n",
    "\n",
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print( ' Message published successfully.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka', ex)\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7fa23f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1166531635.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [32]\u001b[1;36m\u001b[0m\n\u001b[1;33m    .option(\"kafka.bootstrap.servers\",\"localhost:9092\") \\\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "dfs \\\n",
    ".write  \\\n",
    ".format(\"KAFKA\")\n",
    ".option(\"kafka.bootstrap.servers\",\"localhost:9092\") \\\n",
    ".option(\"topic\",\"quickstart-events4\") \\\n",
    ".save()\n",
    "print(\"Data written to Kafka topic 'quickstart-events4'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42629961",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
