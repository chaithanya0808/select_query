{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b8f0e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import  KafkaProducer\n",
    "import sys, os, json\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "import kafka\n",
    "from pyspark.sql.functions import to_json, col, struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c473c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_message(producer_instance, topic_name, key, value):\n",
    "    try:\n",
    "        key_bytes = bytes(key, encoding='utf-8')\n",
    "        value_bytes = bytes(value, encoding='utf-8')\n",
    "        producer_instance.send(topic_name, key=key_bytes, value=value_bytes)\n",
    "        producer_instance.flush()\n",
    "        print( ' Message published successfully.')\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message')\n",
    "        print(str(ex))\n",
    "\n",
    "\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_server=['localhost:9092'], api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka', ex)\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93e66d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'quickstart-events4'\n",
    "bootstrap_server=\"localhost:9092\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25d1288e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using packages ['org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0', 'org.apache.kafka:kafka-clients:3.5.1']\n"
     ]
    }
   ],
   "source": [
    "scala_version = '2.12'  # TODO: Ensure this is correct\n",
    "spark_version = '3.3.0'\n",
    "\n",
    "packages = [\n",
    "    f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}',\n",
    "    'org.apache.kafka:kafka-clients:3.5.1'\n",
    "]\n",
    "\n",
    "args = os.environ.get('PYSPARK_SUBMIT_ARGS', '')\n",
    "if not args:\n",
    "    args = f'--packages {\",\".join(packages)}'\n",
    "    print('Using packages', packages)\n",
    "    os.environ['PYSPARK_SUBMIT_ARGS'] = f'{args} pyspark-shell'\n",
    "else:\n",
    "    print(f'Found existing args: {args}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfb2c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Java home\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\Program Files\\Java\\jdk1.8.0_251\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9c1d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setAppName(\"ETLPipeline\") \\\n",
    "    .setMaster(\"local\") \\\n",
    "    .set(\"spark.driver.extraClassPath\",\"c:/pyspark/*\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "etl = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc565522",
   "metadata": {},
   "outputs": [],
   "source": [
    "uid=\"chaithanya\"\n",
    "pwd=\"12345678\"\n",
    "\n",
    "#sql db details\n",
    "server = \"localhost\"\n",
    "src_db = \"prac\"\n",
    "src_driver = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fed45a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_url = f\"jdbc:sqlserver://{server}:1433;databaseName={src_db};user={uid};password={pwd};\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a622dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"select emp_id,emp_name,salary,manager_id from prac.dbo.emp_manager\"\"\"\n",
    "table=\"emp_manager\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f70526ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+------+----------+\n",
      "|emp_id|emp_name|salary|manager_id|\n",
      "+------+--------+------+----------+\n",
      "|     1|   Ankit| 10000|         4|\n",
      "|     2|   Mohit| 15000|         5|\n",
      "|     3|   Vikas| 10000|         4|\n",
      "|     4|   Rohit|  5000|         2|\n",
      "|     5|   Mudit| 12000|         6|\n",
      "|     6|    Agam| 12000|         2|\n",
      "|     7|  Sanjay|  9000|         2|\n",
      "|     8|  Ashish|  5000|         2|\n",
      "+------+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfs=etl.read. \\\n",
    "    format(\"jdbc\"). \\\n",
    "    options(driver=src_driver, user=uid, password=pwd, url=src_url,query=query). \\\n",
    "    load()\n",
    "dfs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a2aa4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception while connecting Kafka Unrecognized configs: {'bootstrap_server': ['localhost:9092']}\n",
      "Unrecognized configs: {'bootstrap_server': ['localhost:9092']}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'iterrows'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m kafka_producer \u001b[38;5;241m=\u001b[39m connect_kafka_producer()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#  iterate over the dataframe and send rows as the value, and keys can be the desired column\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m \u001b[43mkafka_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterrows\u001b[49m(): \n\u001b[0;32m     10\u001b[0m     publish_message(kafka_producer, topic_name, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kafka_producer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\Sparkbinaryfiles\\spark-3.3.0-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1978\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[0;32m   1979\u001b[0m \n\u001b[0;32m   1980\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1985\u001b[0m \u001b[38;5;124;03m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[0;32m   1986\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m-> 1988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1989\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[0;32m   1990\u001b[0m     )\n\u001b[0;32m   1991\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[0;32m   1992\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'iterrows'"
     ]
    }
   ],
   "source": [
    "kafka_df = dfs.selectExpr(\"CAST(emp_id AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "from datetime import datetime, timedelta\n",
    "#pd_df = dfs.toPandas()\n",
    "#  Reads data from sql file and published those  data as json format to the topic \n",
    "\n",
    "kafka_producer = connect_kafka_producer()\n",
    "\n",
    "#  iterate over the dataframe and send rows as the value, and keys can be the desired column\n",
    "for index, row in kafka_df.iterrows(): \n",
    "    publish_message(kafka_producer, topic_name, \"key\",  \"value\") \n",
    "    \n",
    "import pandas as pd\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "pandasDF = df.toPandas()\n",
    "for index, row in pandasDF.iterrows():   \n",
    "    publish_message(kafka_producer, topic_name, \"key\",  \"value\") \n",
    "    \n",
    "if kafka_producer is not None:\n",
    "    kafka_producer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be260fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
